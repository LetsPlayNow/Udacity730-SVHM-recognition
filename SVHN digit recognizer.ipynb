{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try our recognizer on SVHN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure size which we will use for resizing all images  \n",
    "Let's find average size and ratio of images. maybe we will resize to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load images and check their average size\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_folder = 'SVHN_data'\n",
    "train_folder = os.path.join(dataset_folder, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ration is: 2.2422181523246274\n",
      "This means if y is 28, then x is 128.28546194838566\n"
     ]
    }
   ],
   "source": [
    "# Let's look for average ratio in train dataset\n",
    "av_x, av_y = 99,47 # These values were obtained from random image\n",
    "train_names = os.listdir(train_folder)\n",
    "i = 1\n",
    "image_extension = 'png'\n",
    "for name in train_names:\n",
    "    is_image = name[-3:] == image_extension\n",
    "    if is_image:\n",
    "        image_filepath = os.path.join(dataset_folder, 'train', name)\n",
    "        image = Image.open(image_filepath)\n",
    "        x, y = image.size\n",
    "        av_x = (av_x * i + x) / (i+1)\n",
    "        av_y = (av_y * i + y) / (i+1)\n",
    "        i += 1\n",
    "        image.close()\n",
    "        \n",
    "ratio = av_x / av_y\n",
    "print('Average ration is:', ratio)\n",
    "print('This means if y is 28, then x is', av_y * ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average size in train dataset is (128.28546194838566, 57.213639901802274)\n"
     ]
    }
   ],
   "source": [
    "print('Average size in train dataset is ({}, {})'.format(av_x, av_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's look at image with this ratio and play with size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[156, 70]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJwAAABGCAIAAAC2f2pTAAASdElEQVR4nO2dyZIjOXKGv0AsXHKp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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=156x70 at 0x7FB531542E10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(image_filepath)\n",
    "new_size = [int(ratio*70), 70]\n",
    "print(new_size)\n",
    "img.resize(new_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe later try to work with them. No I will choose this small size (it's a little bit bigger than in synthesized dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[h:70, w:156]\n"
     ]
    }
   ],
   "source": [
    "ratio = 2.24223107819886\n",
    "image_height = 70\n",
    "image_width = int(ratio * image_height)\n",
    "print('[h:{}, w:{}]'.format(image_height, image_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I wonder. Will it be hard for dictionary to contain too much info?  \n",
    "  Will be better here to use variables instead of that. Because it doesn't looks simple anyway.  \n",
    "  **Try to check it later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratio = 2.24223107819886\n",
    "image_height = 70\n",
    "image_width = int(ratio * image_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform our datasets into objects with nice API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def normalize(arr):\n",
    "    arr=arr.astype('float32', copy=False)\n",
    "    if arr.max() > 1.0:\n",
    "        arr/=255.0\n",
    "    return arr\n",
    "\n",
    "class Dataset:\n",
    "    num_classes = 11 # 10 [0-9] digits and 11 for empty digit label\n",
    "    no_digit_label = 10 # 11-th digit (which is 10) doesn't exist\n",
    "    digits_limit = 5\n",
    "    \n",
    "    \"\"\"Represents part of SVHN dataset. Contain images of required size and labels\"\"\"\n",
    "    def __init__(self, name, size, images, labels, image_size):\n",
    "        self.name = name\n",
    "        self.size = size\n",
    "        self.images = images\n",
    "        self.image_size = image_size\n",
    "        self.labels = labels \n",
    "        \n",
    "        self.idx = 0\n",
    "    \n",
    "    @classmethod\n",
    "    def from_directory(cls, name, folder_path, labels_file_path, image_size):\n",
    "        with open(labels_file_path, 'rb') as labels_file:\n",
    "            dataset_info = pickle.load(labels_file)\n",
    "        \n",
    "        # this hack for one image with 6 digits\n",
    "        if name == 'train':\n",
    "            del dataset_info['29930.png']\n",
    "        \n",
    "        size = len(dataset_info)\n",
    "        image_size = image_size\n",
    "        width, height = image_size\n",
    "        images = np.empty([size, height, width, 3], dtype='uint8')\n",
    "        labels = np.full([size, Dataset.digits_limit], Dataset.no_digit_label)\n",
    "        \n",
    "        i = 0\n",
    "        for image_file_name, boxes in dataset_info.items():\n",
    "            # Add next image\n",
    "            image_path = os.path.join(folder_path, image_file_name)\n",
    "            with Image.open(image_path) as image:\n",
    "                resized_image = image.resize(image_size)\n",
    "                images[i] = normalize(np.array(resized_image))\n",
    "            \n",
    "            # Add next label\n",
    "            for j, box in enumerate(boxes):\n",
    "                labels[i][j] = box['label']\n",
    "            \n",
    "            i+=1\n",
    "            \n",
    "        ds = Dataset(name, size, images, labels, image_size)\n",
    "        return ds\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pickle(cls, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "        return dataset\n",
    "    \n",
    "    def to_pickle(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def next_batch(self, size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size(int): size of batch\n",
    "            \n",
    "        Returns:\n",
    "            images(np.ndarray): images. shape [size, height, width, 3]\n",
    "            labels(np.ndarray): labels for classifiers. shape [digits_limit, size]\n",
    "            digits_counts(np.array): labels for counts_classifier. shape [size]\n",
    "        \"\"\"\n",
    "        \n",
    "        width, height = self.image_size\n",
    "        images = np.empty([size, height, width, 3])\n",
    "        labels = np.empty([size, self.digits_limit])\n",
    "        digits_counts = np.empty([size])\n",
    "        \n",
    "        for i in range(size):\n",
    "            images[i] = self.images[self.idx]\n",
    "            labels[i] = self.labels[self.idx]\n",
    "            \n",
    "            digits_count = 0\n",
    "            for digit in labels[i]:\n",
    "                if digit != self.no_digit_label:\n",
    "                    digits_count += 1\n",
    "            digits_counts[i] = digits_count\n",
    "            \n",
    "            self.idx = (self.idx + 1) % self.size\n",
    "        \n",
    "        # Reshape labels for classifiers\n",
    "        labels = labels.T\n",
    "        digits_counts = digits_counts.reshape([-1, 1])\n",
    "        return images, labels, digits_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write dataset to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_folder = 'SVHN_data'\n",
    "dataset_names = ['train', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio = 2.24223107819886\n",
    "image_height = 70\n",
    "image_size = (int(ratio * image_height), image_height) # [156, 70]. Look previous section for details\n",
    "\n",
    "for name in dataset_names:\n",
    "    folder_path = os.path.join(dataset_folder, name)\n",
    "    labels_file_path = os.path.join(folder_path, name+'.pickle')\n",
    "    dataset = Dataset.from_directory(name, folder_path, labels_file_path, image_size)\n",
    "    \n",
    "    dataset_file = os.path.join(dataset_folder, name+'.pickle')\n",
    "    dataset.to_pickle(dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move it to SVHM dataset cracking notebook.    \n",
    "Create batch generator oooooor implement needed methods in Dataset class (need slightly change format of labels (one hot encoding)  \n",
    "Start learning model with this data.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_folder = 'SVHN_data'\n",
    "dataset_names = ['train', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for name in dataset_names:\n",
    "    dataset_file = os.path.join(dataset_folder, name+'.pickle')\n",
    "    datasets.append(Dataset.from_pickle(dataset_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And don't forget about validation\n",
    "def split_dataset(dataset, ratio):\n",
    "    \"\"\"\n",
    "    Split dataset in two by ratio\n",
    "    \n",
    "    Args:\n",
    "        dataset(Dataset): dataset to split in two\n",
    "        ratio(float in range [0, 1]): which part of source dataset will become first in split\n",
    "        \n",
    "    Returns:\n",
    "        datasets (Dataset, Dataset)\n",
    "    \"\"\"\n",
    "    \n",
    "    size = dataset.size\n",
    "    first_size = int(ratio * size)\n",
    "    second_size = size - first_size\n",
    "    \n",
    "    dataset_1 = Dataset(\"\", first_size, dataset.images[:first_size], \n",
    "                        dataset.labels[:first_size], dataset.image_size)\n",
    "    \n",
    "    dataset_2 = Dataset(\"\", second_size, dataset.images[first_size:], \n",
    "                        dataset.labels[first_size:], dataset.image_size)\n",
    "    \n",
    "    return dataset_1, dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = split_dataset(datasets[0], 0.7)\n",
    "test_dataset = datasets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and train our model from 'MNIST numbers recognizer.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Operations wrappers\n",
    "def weights_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.2)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.zeros(shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class LinearModel(ABC):\n",
    "    '''As we have a graph, we dont need methods like\n",
    "        def logits(self, x):\n",
    "        def loss(self, x, y):\n",
    "        def predictions(self, x):\n",
    "        \n",
    "        because we don't really passing data to these tensors\n",
    "        we just passing tensors, where this data will be again and again (they still the same)\n",
    "        \n",
    "        instead of that we can connect tensors in right way once\n",
    "        and save resulting tensors in fields\n",
    "        After that we can execute them in a session easier\n",
    "        '''\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __init__(self, num_features, num_classes, x, y):  \n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.w = weights_variable([num_features, num_classes])\n",
    "        self.b = bias_variable([num_classes])\n",
    "        \n",
    "        self.logits = tf.matmul(x, self.w) + self.b\n",
    "        self.optimizer = tf.train.AdagradOptimizer(learning_rate = 0.007)\n",
    "         \n",
    "class LinearClassifier(LinearModel):\n",
    "    def __init__(self,  num_features, num_classes, x, y):\n",
    "        LinearModel.__init__(self, num_features, num_classes, x, y)\n",
    "        \n",
    "        # Loss\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=y)\n",
    "        self.loss = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "        # Probabilities and predictions\n",
    "        self.probabilities = tf.nn.softmax(self.logits)\n",
    "        self.predictions = tf.argmax(self.probabilities, axis=1)\n",
    "        \n",
    "        # Accuracy\n",
    "        labels = y\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predictions, labels), tf.float32))\n",
    "        \n",
    "        # Training\n",
    "        self.train_step = self.optimizer.minimize(self.loss)\n",
    "    \n",
    "class LinearRegression(LinearModel):\n",
    "    def __init__(self,  num_features, num_classes, x, y):\n",
    "        LinearModel.__init__(self, num_features, 1, x, y)\n",
    "        \n",
    "        # Loss\n",
    "        mse = tf.reduce_mean(tf.squared_difference(self.logits, y))\n",
    "        self.loss = mse\n",
    "        \n",
    "        # Predictions\n",
    "        self.predictions = self.logits # to int and the calculate accuracy!\n",
    "        \n",
    "        # Training\n",
    "        self.train_step = self.optimizer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digits_per_image = 5\n",
    "num_classes = 11\n",
    "num_classifiers = digits_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 18, 39, 64) 44928\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # conv > pool > conv > pool > fc1 > dropout > (5 or more linear classifiers and 1 for digits count)\n",
    "    x = tf.placeholder(tf.float32, shape=[None, image_height, image_width, 3])\n",
    "    ys = tf.placeholder(tf.int64, shape=[digits_per_image, None])\n",
    "    y_cnt = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    \n",
    "    W_conv1 = weights_variable([5, 5, 3, 32])\n",
    "    b_conv1 = bias_variable(32)\n",
    "    \n",
    "    h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool2x2(h_conv1)\n",
    "    \n",
    "    W_conv2 = weights_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable(64)\n",
    "    \n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool2x2(h_conv2)\n",
    "    \n",
    "    # A little bit hacky.\n",
    "    # I should learn to calc conv shapes in mind on fly\n",
    "    flat_size = int(h_pool2.shape[1] * h_pool2.shape[2] * h_pool2.shape[3])\n",
    "    print(h_pool2.shape, flat_size)\n",
    "    \n",
    "    fc1_size = 1024\n",
    "    W_fc1 = weights_variable([flat_size, fc1_size])\n",
    "    b_fc1 = bias_variable([fc1_size])\n",
    "    \n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, flat_size])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    dropout = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    image_embeds = dropout\n",
    "    \n",
    "    # Digits classifiers\n",
    "    # ================================================================\n",
    "    digits_classifiers = []\n",
    "    for i in range(num_classifiers):\n",
    "        i_th_digit_classifier = LinearClassifier(fc1_size, num_classes, image_embeds, ys[i])\n",
    "        digits_classifiers.append(i_th_digit_classifier)\n",
    "    \n",
    "    # Digits count classifier\n",
    "    count_classifier = LinearRegression(fc1_size, num_classes, image_embeds, y_cnt)\n",
    "    top_classifiers = digits_classifiers + [count_classifier]\n",
    "    \n",
    "    # Summary loss\n",
    "    summary_loss = 0\n",
    "    for c in top_classifiers: # or in digits_classifiers only?\n",
    "        summary_loss += c.loss\n",
    "    average_loss = summary_loss / (num_classifiers + 1)\n",
    "    \n",
    "    summary_train_step = tf.train.AdamOptimizer(learning_rate=0.007).minimize(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "Average loss 28763.802734375\n",
      "0th digit classifier: [accuracy: 0.006666666828095913, loss: 35.8723258972168]\n",
      "1th digit classifier: [accuracy: 0.07999999821186066, loss: 36.61647033691406]\n",
      "2th digit classifier: [accuracy: 0.7266666889190674, loss: 18.594942092895508]\n",
      "3th digit classifier: [accuracy: 0.006666666828095913, loss: 28.696243286132812]\n",
      "4th digit classifier: [accuracy: 0.07000000029802322, loss: 13.799060821533203]\n",
      "Count classifier loss: 172449.234375\n",
      "step 10\n",
      "step 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5b3187a30145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtraining_approach\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"use summary loss\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_train_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtraining_approach\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train separately\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py_36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py_36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py_36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py_36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py_36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 2000\n",
    "batch_size = 64\n",
    "training_approach = \"use summary loss\"\n",
    "validation_size = 300 # todo choose it right\n",
    "test_size = 300\n",
    "print_size = 10\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Arrays to train all in one session\n",
    "    train_steps = [c.train_step for c in top_classifiers]\n",
    "    losses = [c.loss for c in digits_classifiers]\n",
    "    accuracies = [c.accuracy for c in digits_classifiers]\n",
    "    tensors_to_run = None\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        images, labels, digits_count = train_dataset.next_batch(batch_size)\n",
    "        feed_dict = {\n",
    "            x: images,\n",
    "            ys: labels,\n",
    "            y_cnt: digits_count,\n",
    "            keep_prob: 0.5\n",
    "        }\n",
    "  \n",
    "        # Training\n",
    "        if training_approach == \"use summary loss\":\n",
    "            session.run(summary_train_step, feed_dict=feed_dict)\n",
    "        elif training_approach == \"train separately\":\n",
    "            session.run(train_steps, feed_dict=feed_dict)\n",
    "\n",
    "        if i % 10 == 0: \n",
    "            print(f'step {i}')\n",
    "\n",
    "        # Validation\n",
    "        if i % 250 == 0:\n",
    "            images, labels, digits_count = validation_dataset.next_batch(validation_size)\n",
    "            feed_dict = {\n",
    "                x: images,\n",
    "                ys: labels,\n",
    "                y_cnt: digits_count,\n",
    "                keep_prob: 1.0\n",
    "            }\n",
    "\n",
    "            if tensors_to_run is None:\n",
    "                tensors_to_run = [losses, accuracies, count_classifier.loss, average_loss]\n",
    "\n",
    "            cls_l, cls_a, cnt_cls_l, a_l = session.run(tensors_to_run, feed_dict=feed_dict)\n",
    "            print(f\"Average loss {a_l}\")\n",
    "            for j in range(num_classifiers):\n",
    "                print(f\"{j}th digit classifier: [accuracy: {cls_a[j]}, loss: {cls_l[j]}]\")\n",
    "            print(f\"Count classifier loss: {cnt_cls_l}\")\n",
    "          \n",
    "    # we can also continue session later\n",
    "    # https://stackoverflow.com/questions/41037650/how-to-restore-session-in-tensorflow\n",
    "    # https://www.tensorflow.org/versions/r0.12/api_docs/python/client/session_management   \n",
    "    # test\n",
    "    print(f'Test for {test_size} images')\n",
    "    images, labels, digits_count = test_dataset.next_batch(test_size)\n",
    "    feed_dict = {\n",
    "        x: images,\n",
    "        ys: labels,\n",
    "        y_cnt: digits_count,\n",
    "        keep_prob: 1.0\n",
    "    }\n",
    "    \n",
    "    # Accuracy\n",
    "    if tensors_to_run is None:\n",
    "        tensors_to_run = [losses, accuracies, count_classifier.loss, average_loss]\n",
    "\n",
    "    cls_l, cls_a, cnt_cls_l, a_l = session.run(tensors_to_run, feed_dict=feed_dict)\n",
    "    print(f\"Average loss {a_l}\")\n",
    "    for j in range(num_classifiers):\n",
    "        print(f\"{j}th digit classifier: [accuracy: {cls_a[j]}, loss: {cls_l[j]}]\")\n",
    "    print(f\"Count classifier loss: {cnt_cls_l}\")\n",
    "\n",
    "    # Actual predictions\n",
    "    print(\"Some samples\")\n",
    "    predictions = [c.predictions for c in top_classifiers]\n",
    "    \n",
    "    c_ps = session.run(predictions, feed_dict=feed_dict)\n",
    "    predicted_digits = c_ps[:digits_per_image]\n",
    "    for i in range(print_size):\n",
    "        plt.imshow(images[i].squeeze())\n",
    "        plt.show()\n",
    "        \n",
    "        number = ''\n",
    "        for j in range(num_classifiers):\n",
    "            digit = predicted_digits[j][i]\n",
    "            if digit != num_classes-1: # 'no digit' encoded as max digit\n",
    "                number += str(digit)\n",
    "                \n",
    "        predicted_length = c_ps[-1]\n",
    "        print('Predicted {} length of {}'.format(number, predicted_length[i]))\n",
    "        \n",
    "    # Save trained model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session, \"/tmp/model.ckpt\")\n",
    "    \n",
    "    # We can restore them later. Just read \n",
    "    # https://www.tensorflow.org/programmers_guide/saved_model"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py_36]",
   "language": "python",
   "name": "conda-env-py_36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
